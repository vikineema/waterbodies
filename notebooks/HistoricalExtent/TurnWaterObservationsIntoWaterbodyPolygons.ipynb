{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cb8795-d0a3-4e40-a9e0-d109d076a302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Path to env file containing the waterbodies database credentials\n",
    "# Only necessary on the Sandbox.\n",
    "dotenv_path = \"/home/jovyan/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path, verbose=True, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bb740-4bd5-47b6-8e31-132501fca671",
   "metadata": {},
   "source": [
    "# Turn water observations into waterbody polygons\n",
    "\n",
    "* **Products used:** \n",
    "[wofs_ls_summary_alltime](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime)\n",
    "* **Prerequisites:** \n",
    "    * The Hydrosheds version 1.1 Land Mask split into [WaterBodiesGrid tiles](../HydroSHEDSv1.1LandMask/SplitHydroSHEDSv1.1LandMaskIntoTiles.ipynb) and saved into a single directory.\n",
    "        * Variable name: `land_sea_mask_rasters_directory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12672abb-9d9b-4043-802e-e050f82c927e",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Water is among one the most precious natural resources and is essential for the survival of life on Earth. For many countries in Africa, the scarcity of water is both an economic and social issue. Water is required not only for consumption but for industries and environmental ecosystems to function and flourish. \n",
    "\n",
    "With the demand for water increasing, there is a need to better understand our water availability to ensure we are managing our water resources effectively and efficiently.  \n",
    "\n",
    "Digital Earth Africa (DE Africa)'s [Water Observations from Space (WOfS) dataset](https://docs.digitalearthafrica.org/en/latest/data_specs/Landsat_WOfS_specs.html), provides a water classified image of Africa approximately every 16 days. These individual water observations have been combined into a [WOfS All-Time Summary](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime) product, which calculates the frequency of wet observations (compared against all clear observations of that pixel), over the full 30-plus years satellite archive. \n",
    "\n",
    "The WOfS All-Time Summary product provides valuable insights into the persistence of water across the African landscape on a pixel by pixel basis. While knowing the wet history of a single pixel within a waterbody is useful, it is more useful to be able to map the whole waterbody as a single object. \n",
    "\n",
    "This notebook demonstrates a workflow for mapping waterbodies across Africa as polygon objects. This workflow has been used to produce **DE Africa Waterbodies**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab9408-3bd7-49d5-bc76-30edb2afb875",
   "metadata": {},
   "source": [
    "## Description\n",
    "This code follows the following workflow:\n",
    "\n",
    "* Load the required python packages\n",
    "* Set your chosen analysis parameters:\n",
    "    * set up the analysis region\n",
    "    * set up some file names for the inputs and outputs\n",
    "    * wetness threshold/s\n",
    "    * minimum number of valid observations\n",
    "    * min/max waterbody size\n",
    "    * read in a land/sea mask\n",
    "* Generate waterbody polygons for each tile:\n",
    "  * For each tile:\n",
    "    * Load the WOfS All Time Summary Dataset\n",
    "    * Mask the dataset using the land/sea mask \n",
    "    * Keep only pixels observed at least x times\n",
    "    * Keep only pixels identified as wet at least x% of the time\n",
    "        * Here the code can take in two wetness thresholds, to produce two initial temporary polygon files.\n",
    "    * Convert the raster data into polygons\n",
    "    * Write the polygons to disk\n",
    "* Remove artificial polygon borders created at tile boundaries by merging polygons that intersect across tile boundaries\n",
    "* Filter the combined polygon dataset (note that this step happens after the merging of tile boundary polygons to ensure that artifacts are not created by part of a polygon being filtered out, while the remainder of the polygon that sits on a separate tile is treated differently).\n",
    "    * Add the area and perimeter and length of each polygon as attributes\n",
    "    * Filter the polygons based on area / size and length\n",
    "    * Add unique IDs to the polygons\n",
    "* Save out the final polygon set to a shapefile or write to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da3758-a279-4c10-adb9-b78ac2ce4883",
   "metadata": {},
   "source": [
    "## Load python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3eada8-2a23-421b-ba6a-481055bf734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "from itertools import chain\n",
    "\n",
    "import click\n",
    "import geohash as gh\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datacube import Datacube\n",
    "from geopandas import gpd\n",
    "from odc.geo.geom import Geometry\n",
    "from shapely.ops import unary_union\n",
    "from waterbodies.db import get_waterbodies_engine\n",
    "from waterbodies.grid import WaterbodiesGrid\n",
    "from waterbodies.historical_extent import (\n",
    "    add_waterbodies_polygons_to_db,\n",
    "    get_polygon_length,\n",
    "    get_waterbodies,\n",
    ")\n",
    "from waterbodies.hopper import create_tasks_from_datasets\n",
    "from waterbodies.io import (\n",
    "    check_directory_exists,\n",
    "    check_file_exists,\n",
    "    find_parquet_files,\n",
    "    get_filesystem,\n",
    ")\n",
    "from waterbodies.logs import logging_setup\n",
    "from waterbodies.text import format_task, get_tile_index_str_from_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3e618-3141-4af4-84c2-419b1e3117fb",
   "metadata": {},
   "source": [
    "## Define Analysis Parameters\n",
    "\n",
    "The following section walks you through the analysis parameters you will need to set for this workflow. Each section describes the parameter, how it is used, and what value was used for the DE Africa Waterbodies product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3076c0-cf69-47b8-9a57-58c81f8d8233",
   "metadata": {},
   "source": [
    "### Set the analysis region\n",
    "If you would like to perform the analysis for all of Africa, using the published WOfS All-time Summary, set `all_of_africa = True`. If you set the flag `all_of_africa` to `False`, you will need to provide a path to the shapefile / GeoJSON defining the area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff98ec-0b79-486c-afb9-933301f11fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_of_africa = False\n",
    "if all_of_africa:\n",
    "    tile_index_filter = None\n",
    "else:\n",
    "    aoi_file = \"https://deafrica-waterbodies-dev.s3.af-south-1.amazonaws.com/waterbodies/v0.0.2/senegal_basin/senegal_basin_boundary.geojson\"\n",
    "    gridspec = WaterbodiesGrid().gridspec\n",
    "    aoi_gdf = gpd.read_file(aoi_file).to_crs(gridspec.crs)\n",
    "    aoi_tiles = list(gridspec.tiles_from_geopolygon(geopolygon=Geometry(geom=aoi_gdf.geometry.iloc[0], crs=aoi_gdf.crs)))\n",
    "    tile_index_filter = [tile[0] for tile in aoi_tiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3291b931-8927-46b3-bd7e-03f7f0673031",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Parse the GeoJSON data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m geojson_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureCollection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m   ]\n\u001b[1;32m     39\u001b[0m }\n\u001b[0;32m---> 40\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeojson_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Parse the GeoJSON data\n",
    "geojson_data = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              -1.5381331338635675,\n",
    "              6.602134444266824\n",
    "            ],\n",
    "            [\n",
    "              -1.5381331338635675,\n",
    "              6.373530335543478\n",
    "            ],\n",
    "            [\n",
    "              -1.267246532216319,\n",
    "              6.373530335543478\n",
    "            ],\n",
    "            [\n",
    "              -1.267246532216319,\n",
    "              6.602134444266824\n",
    "            ],\n",
    "            [\n",
    "              -1.5381331338635675,\n",
    "              6.602134444266824\n",
    "            ]\n",
    "          ]\n",
    "        ],\n",
    "        \"type\": \"Polygon\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "features = json.dumps(geojson_data)['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839551a-36fd-4d74-b1a9-c4f6d1aa9e44",
   "metadata": {},
   "source": [
    "### Set up the directory to save the waterbodies for each tile\n",
    "\n",
    "When overwrite is set to `False` if a waterbodies polygons file exists in the `output_directory` for a specific tile, the tile is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba3bbc3-9643-4e6d-8539-7c4a689a44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"s3://deafrica-waterbodies-dev/waterbodies/v0.0.2/intermediate_outputs\"\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd76f66-da5b-4176-80fc-96e329a3df6b",
   "metadata": {},
   "source": [
    "<a id='wetnessthreshold'></a>\n",
    "### How frequently wet does a pixel need to be to be included?\n",
    "The values set here will be the minimum frequency (as a decimal between 0 and 1) that you want water to be detected across all analysis years before it is included. \n",
    "\n",
    "E.g. If this was set to 0.10, any pixels that are wet *at least* 10% of the time across all valid observations will be included.\n",
    "\n",
    "Polygons identified by the extent threshold that intersect with the polygons generated by the detection threshold will be extracted, and included in the final polygon dataset. This means that the **location** of polygons is set by the detection threshold, but the **shape/extent** of these polygons is set by the extent threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f3e0a-b6d7-436a-98f6-2f0664aaa95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.1\n",
    "extent_threshold = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0b3ea-b8f2-4496-9135-437e3cf65cdb",
   "metadata": {},
   "source": [
    "<a id='size'></a>\n",
    "\n",
    "### How big/small should the polygons be?\n",
    "This filtering step can remove very small waterbodies and very large waterbodies will be segmented. The size listed here is in pixels.\n",
    "> Note: A single pixel in Landsat data is 30 m x 30 m = 900 m<sup>2</sup>. \n",
    "\n",
    "**MinSize**\n",
    "\n",
    "E.g. A minimum size of 6 pixels means that polygons 5 pixels or less will be excluded. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "**MaxSize**\n",
    "\n",
    "E.g. A maximum size of 1000 pixels means that if a polygon is larger than 1000 pixels the polygon will segmented using watershed segmentation. If you don't want to use this filter, set this number to `math.inf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6581d-3015-45af-8462-924cc599b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_polygon_size = 6\n",
    "max_polygon_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ff77d-9984-477d-8427-d77c5bbed9ea",
   "metadata": {},
   "source": [
    "### Filter results based on number of valid observations\n",
    "\n",
    "The total number of valid WOfS observations for each pixel varies depending on the frequency of clouds and cloud shadow, the proximity to high slope and terrain shadow, and the seasonal change in solar angle. \n",
    "\n",
    "The `count_clear` parameter within the [`wofs_ls_summary_alltime`](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime) data provides a count of the number of valid observations each pixel recorded over the analysis period. We can use this parameter to mask out pixels that were infrequently observed. \n",
    "If this mask is not applied, pixels that were observed only once could be included if that observation was wet (i.e. a single wet observation means the calculation of the frequency statistic would be (1 wet observation) / (1 total observation) = 100% frequency of wet observations).\n",
    "\n",
    "Note that this parameter does not specify the timing of these observations, but rather just the **total number of valid observations** (observed at any time of the year, in any year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a0b4f-3b51-44a4-aa1c-4a73c3e9e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_valid_observations = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cc248-64b8-46bb-a879-e0161135e014",
   "metadata": {},
   "source": [
    "<a id='coastline'></a>\n",
    "### Read in a land/sea mask\n",
    "\n",
    "You can choose which land/sea mask you would like to use to mask out ocean polygons, depending on how much coastal water you would like in the final product. We use the [HydroSHEDS version 1.1 Land Mask](https://www.hydrosheds.org/hydrosheds-core-downloads). Any WOfS All Time Summary product pixels with a value of 0 in the land/sea mask are filtered out. \n",
    "\n",
    "To use a different product, supply a directory path to `land_sea_mask_rasters_directory` which contains raster tiles of the land/sea mask covering all the WOfS All Time Summary product [regions](https://explorer.digitalearth.africa/api/regions/wofs_ls_summary_alltime). See the [Split HydroSHEDS v1.1 Land Mask](../HydroSHEDSv1.1LandMask/SplitHydroSHEDSv1.1LandMaskIntoTiles.ipynb) notebook on how to split your raster product into tiles and the naming convention to use when saving the raster tiles. Ensure that for your product, pixels with the value 0 are ocean pixels and pixels with the value 1 are land pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b5d43f-f1f3-4a76-8331-dccf3d505904",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_sea_mask_rasters_directory = \"s3://deafrica-waterbodies-dev/waterbodies/v0.0.2/hydrosheds_v1_1_land_mask/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7747d9-3193-4f9f-bae9-157f58705950",
   "metadata": {},
   "source": [
    "## Generate the waterbody polygons for each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ac6db-ceb4-433d-9071-9894a77a1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "logging_setup(3)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9406d-e1f7-4f95-9a54-9683df9f2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the datacube\n",
    "dc = Datacube(app=\"GeneratePolygons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4f56f-a4c1-4326-8bf9-f09c7e752cc0",
   "metadata": {},
   "source": [
    "### Group all the WOfS All Time Summary product datasets into tiles\n",
    "\n",
    "In an Argo workflow, this is done by the `waterbodies historical-extent generate-tasks` cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91021b9b-f7b9-43cc-a75c-d3773fd823d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the WOfS All Time Summaries datasets\n",
    "dc_query = dict(product=\"wofs_ls_summary_alltime\")\n",
    "datasets = dc.find_datasets(**dc_query)\n",
    "_log.info(f\"Found {len(datasets)} datasets matching the query {dc_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51385c64-c377-4a2c-8527-da0e3de1f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each task contains a tile index and the dataset UUID of the WOfS All Time Summary dataset\n",
    "# that covers the tile.\n",
    "tasks = create_tasks_from_datasets(datasets=datasets, tile_index_filter=tile_index_filter, bin_solar_day=False)\n",
    "tasks = [format_task(task) for task in tasks]\n",
    "sorted_tasks = sorted(tasks, key=lambda x: x[\"tile_index_x\"])\n",
    "_log.info(f\"Total number of tasks: {len(sorted_tasks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d17608-e06a-42a3-9f91-43074bb2755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the tasks to a text file.\n",
    "max_parallel_steps = 1\n",
    "task_chunks = np.array_split(np.array(sorted_tasks), max_parallel_steps)\n",
    "task_chunks = [chunk.tolist() for chunk in task_chunks]\n",
    "task_chunks = list(filter(None, task_chunks))\n",
    "task_chunks_count = str(len(task_chunks))\n",
    "_log.info(f\"{len(sorted_tasks)} tasks chunked into {task_chunks_count} chunks\")\n",
    "task_chunks_json_array = json.dumps(task_chunks)\n",
    "\n",
    "tasks_directory = \"/tmp/\"\n",
    "tasks_output_file = os.path.join(tasks_directory, \"tasks_chunks\")\n",
    "tasks_count_file = os.path.join(tasks_directory, \"tasks_chunks_count\")\n",
    "\n",
    "fs = get_filesystem(path=tasks_directory, anon=False)\n",
    "\n",
    "if not check_directory_exists(path=tasks_directory):\n",
    "    fs.mkdirs(path=tasks_directory, exist_ok=True)\n",
    "    _log.info(f\"Created directory {tasks_directory}\")\n",
    "\n",
    "with fs.open(tasks_output_file, \"w\") as file:\n",
    "    file.write(task_chunks_json_array)\n",
    "_log.info(f\"Tasks chunks written to {tasks_output_file}\")\n",
    "\n",
    "with fs.open(tasks_count_file, \"w\") as file:\n",
    "    file.write(task_chunks_count)\n",
    "_log.info(f\"Tasks chunks count written to {tasks_count_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51966b-9f63-479b-b132-d467082528d6",
   "metadata": {},
   "source": [
    "### Loop over each tile and generate the waterbodies for the tile\n",
    "\n",
    "In an Argo workflow, this is done by `waterbodies historical-extent process-tasks` cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7ccc0-e719-4e96-907c-bf2042886c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_list_file = \"/tmp/tasks_chunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28172e41-4194-4c6e-af71-49172c1cec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_directory_exists(path=land_sea_mask_rasters_directory):\n",
    "    e = FileNotFoundError(f\"Directory {land_sea_mask_rasters_directory} does not exist!\")\n",
    "    _log.error(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d3021-80de-49be-b60f-b7bded4848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of tiles we generated in the previous step.\n",
    "fs = get_filesystem(path=tasks_list_file, anon=True)\n",
    "with fs.open(tasks_list_file) as file:\n",
    "    content = file.read()\n",
    "    decoded_content = content.decode()\n",
    "    tasks = json.loads(decoded_content)\n",
    "\n",
    "# In case file contains list of lists\n",
    "if all(isinstance(item, list) for item in tasks):\n",
    "    tasks = list(chain(*tasks))\n",
    "else:\n",
    "    pass\n",
    "_log.info(f\"Found {len(tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0114f-f5bf-4b33-939e-483e64cbc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_directory_exists(path=output_directory):\n",
    "    fs = get_filesystem(output_directory, anon=False)\n",
    "    fs.mkdirs(output_directory)\n",
    "    _log.info(f\"Created the directory {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ae95b-175e-4cf4-a78d-572f6f48fe15",
   "metadata": {},
   "source": [
    "The next cell takes about 12 hours to run for the entire continent. Therefore for processing for all of Africa please run the `waterbodies historical-extent process-tasks` cli in Argo using parallel pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52e563-89e6-4391-886a-77ebae8b45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_tasks = []\n",
    "for idx, task in enumerate(tasks):\n",
    "    _log.info(f\"Processing task: {task}   {idx+1}/{len(tasks)}\")\n",
    "    tile_index_x = task[\"tile_index_x\"]\n",
    "    tile_index_y = task[\"tile_index_y\"]\n",
    "    task_datasets_ids = task[\"task_datasets_ids\"]\n",
    "\n",
    "    task_id_tuple = (tile_index_x, tile_index_y)\n",
    "    task_id_str = get_tile_index_str_from_tuple(task_id_tuple)\n",
    "    output_file_name = os.path.join(output_directory, f\"waterbodies_{task_id_str}.parquet\")\n",
    "    try:\n",
    "        if not overwrite:\n",
    "            exists = check_file_exists(output_file_name)\n",
    "\n",
    "        if overwrite or not exists:\n",
    "            waterbody_polygons = get_waterbodies(\n",
    "                tile_index_x=tile_index_x,\n",
    "                tile_index_y=tile_index_y,\n",
    "                task_datasets_ids=task_datasets_ids,\n",
    "                dc=dc,\n",
    "                land_sea_mask_rasters_directory=land_sea_mask_rasters_directory,\n",
    "                detection_threshold=detection_threshold,\n",
    "                extent_threshold=extent_threshold,\n",
    "                min_valid_observations=min_valid_observations,\n",
    "                min_polygon_size=min_polygon_size,\n",
    "                max_polygon_size=max_polygon_size\n",
    "            )\n",
    "            if waterbody_polygons.empty:\n",
    "                _log.info(f\"Task {task_id_str} has no waterbody polygons\")\n",
    "            else:\n",
    "                _log.info(\n",
    "                    f\"Task {task_id_str} has {len(waterbody_polygons)} waterbody polygons\"\n",
    "                )\n",
    "                waterbody_polygons.to_parquet(output_file_name)\n",
    "                _log.info(f\"Waterbodies written to {output_file_name}\")\n",
    "        else:\n",
    "            _log.info(f\"Task {task_id_str} already exists, skipping\")\n",
    "    except Exception as error:\n",
    "        _log.exception(error)\n",
    "        _log.error(f\"Failed to process task {task}\")\n",
    "        failed_tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fb0f4-5fd7-4534-92f6-0b94373e7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed_tasks:\n",
    "    _log.info(f\"The following tasks failed: {failed_tasks}\")\n",
    "    failed_tasks_json_array = json.dumps(failed_tasks)\n",
    "\n",
    "    tasks_directory = \"/tmp/\"\n",
    "    failed_tasks_output_file = os.path.join(tasks_directory, \"failed_tasks\")\n",
    "\n",
    "    fs = get_filesystem(path=tasks_directory, anon=False)\n",
    "\n",
    "    if not check_directory_exists(path=tasks_directory):\n",
    "        fs.mkdirs(path=tasks_directory, exist_ok=True)\n",
    "        _log.info(f\"Created directory {tasks_directory}\")\n",
    "\n",
    "    with fs.open(failed_tasks_output_file, \"a\") as file:\n",
    "        file.write(failed_tasks_json_array + \"\\n\")\n",
    "    _log.info(f\"Failed tasks written to {failed_tasks_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e947d-d3c0-493f-aa33-c4d965a65ae6",
   "metadata": {},
   "source": [
    "## Merge polygons that have an edge at a tile boundary\n",
    "\n",
    "Now that we have all of the polygons across our whole region of interest, we need to check for artifacts in the data caused by tile boundaries.\n",
    "\n",
    "We have created a GeoDataFrame `buffered_30m_tiles`, that consists of the tile boundaries, plus a 1 pixel (30 m) buffer. This GeoDataFrame will help us to find any polygons that have a boundary at the edge of a tile. We can then find where polygons touch across this boundary, and join them up.\n",
    "\n",
    "In an Argo workflow, this is done by `waterbodies historical-extent process-polygons` cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327cf31-8f04-45db-b72c-7f3f3c52062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_directory = output_directory\n",
    "polygons_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7cea6-9981-4170-9429-9b6fc89675e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = find_parquet_files(directory_path=polygons_directory, file_name_pattern=r\".*\")\n",
    "_log.info(f\"Found {len(files)} files containing waterbodies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9508da-a979-4062-853f-c1a3f7549eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load all the waterbodies files into a single GeoDataFrame.\n",
    "gridspec = WaterbodiesGrid().gridspec\n",
    "\n",
    "waterbodies_list = []\n",
    "for file in files:\n",
    "    gdf = gpd.read_file(file).to_crs(gridspec.crs)\n",
    "    waterbodies_list.append(gdf)\n",
    "\n",
    "waterbodies = pd.concat(waterbodies_list, ignore_index=True)\n",
    "_log.info(f\"Loaded {len(waterbodies)} waterbodies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2241ed-c7a9-41fd-9371-c3e15266b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tiles used to generate the waterbodies.\n",
    "datasets = dc.find_datasets(product=\"wofs_ls_summary_alltime\")\n",
    "tasks = create_tasks_from_datasets(\n",
    "    datasets=datasets, tile_index_filter=None, bin_solar_day=False\n",
    ")\n",
    "tile_indices = [k for task in tasks for k, v in task.items()]\n",
    "buffered_tile_boundaries = [\n",
    "    gridspec.tile_geobox(tile_index=tile_index).extent.geom.boundary.buffer(\n",
    "        30, cap_style=\"flat\", join_style=\"mitre\"\n",
    "    )\n",
    "    for tile_index in tile_indices\n",
    "]\n",
    "buffered_tile_boundaries_gdf = gpd.GeoDataFrame(\n",
    "    data={\"tile_index\": tile_indices, \"geometry\": buffered_tile_boundaries}, crs=gridspec.crs\n",
    ")\n",
    "buffered_tile_boundaries_gdf.set_index(\"tile_index\", inplace=True)\n",
    "_log.info(f\"Found {len(buffered_tile_boundaries_gdf)} tiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fe540-3d2e-4824-9b58-f1ec644375c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_log.info(\"Merging waterbodies at tile boundaries...\")\n",
    "joined = gpd.sjoin(\n",
    "    waterbodies, buffered_tile_boundaries_gdf, how=\"inner\", predicate=\"intersects\"\n",
    ")\n",
    "if joined.empty:\n",
    "    pass\n",
    "else:\n",
    "    tile_boundary_waterbodies = waterbodies[waterbodies.index.isin(joined.index)]\n",
    "    not_tile_boundary_waterbodies = waterbodies[~waterbodies.index.isin(joined.index)]\n",
    "    tile_boundary_waterbodies_merged = (\n",
    "        gpd.GeoDataFrame(\n",
    "            crs=gridspec.crs, geometry=[unary_union(tile_boundary_waterbodies.geometry)]\n",
    "        )\n",
    "        .explode(index_parts=True)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    waterbodies = pd.concat(\n",
    "        [not_tile_boundary_waterbodies, tile_boundary_waterbodies_merged],\n",
    "        ignore_index=True,\n",
    "        sort=True,\n",
    "    )\n",
    "_log.info(f\"Waterbodies count after merging waterbodies at tile boundaries: {len(waterbodies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b660ac0-cdc7-496a-9a67-4c9e74986b3c",
   "metadata": {},
   "source": [
    "## Add waterbodies attributes\n",
    "\n",
    "Add the perimeter, area and length attributes to each waterbody.\n",
    "\n",
    "Remove waterbodies that meet the following criteria:\n",
    "- Waterbodies whose area is less than 4500 m<sup>2</sup>\n",
    "- Waterbodies whose length is greater than 150km\n",
    "\n",
    "\n",
    "In an Argo workflow, this is included in the `waterbodies historical-extent process-polygons` cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058d047-58f4-405a-a7dd-7fac72628e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies[\"area_m2\"] = waterbodies.geometry.area\n",
    "waterbodies = waterbodies[waterbodies.area_m2 > 4500]\n",
    "_log.info(\n",
    "    f\"Waterbodies count after filtering out waterbodies smaller than 4500m2: {len(waterbodies)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bccd52-1081-4afa-b21a-d9f65ee9618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies[\"length_m\"] = waterbodies.geometry.apply(get_polygon_length)\n",
    "waterbodies = waterbodies[waterbodies.length_m <= (150 * 1000)]\n",
    "_log.info(\n",
    "    f\"Waterbodies count after filtering out waterbodies longer than than 150km: {len(waterbodies)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb0cb3-b661-4b7a-b4f0-084be20045af",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies[\"perim_m\"] = waterbodies.geometry.length\n",
    "waterbodies = waterbodies.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8601988-d684-49bc-a097-08ba813f0290",
   "metadata": {},
   "source": [
    "## Generate a unique ID for each polygon\n",
    "\n",
    "A unique identifier is required for every polygon to allow it to be referenced. The naming convention for generating unique IDs here is the [geohash](geohash.org).\n",
    "\n",
    "A Geohash is a geocoding system used to generate short unique identifiers based on latitude/longitude coordinates. It is a short combination of letters and numbers, with the length of the string a function of the precision of the location. The methods for generating a geohash are outlined [here - yes, the official documentation is a wikipedia article](https://en.wikipedia.org/wiki/Geohash).\n",
    "\n",
    "Here we use the python package `python-geohash` to generate a geohash unique identifier for each polygon. We use `precision = 10` geohash characters, which represents an on the ground accuracy of <20 metres. This ensures that the precision is high enough to differentiate between waterbodies located next to each other.\n",
    "\n",
    "In an Argo workflow, this is included in the `waterbodies historical-extent process-polygons` cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0150314-d124-4c47-b22a-0c6c39063282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "waterbodies[\"uid\"] = waterbodies.geometry.apply(\n",
    "    lambda x: gh.encode(x.centroid.y, x.centroid.x, precision=10)\n",
    ")\n",
    "assert waterbodies[\"uid\"].is_unique\n",
    "waterbodies.sort_values(by=[\"uid\"], inplace=True)\n",
    "waterbodies.reset_index(inplace=True, drop=True)\n",
    "waterbodies[\"wb_id\"] = waterbodies.index + 1\n",
    "assert waterbodies[\"wb_id\"].min() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163289b-4d99-4382-b3ad-d3f23494508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(f\"Final waterbodies count: {len(waterbodies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56205-ab06-490a-9c7d-76a68dfbe855",
   "metadata": {},
   "source": [
    "## Write the waterbodies to the database\n",
    "\n",
    "In an Argo workflow, this is included in the `waterbodies historical-extent process-polygons` cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc303a97-d861-4ca3-b9b8-7feabcecf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "engine = get_waterbodies_engine()\n",
    "add_waterbodies_polygons_to_db(\n",
    "    waterbodies_polygons=waterbodies, engine=engine, update_rows=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
